{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import math, random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pandas as pd\n",
    "import csv, torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/IceKing.csv', sep=\"|\")\n",
    "\n",
    "cleanup = []\n",
    "for index,row in data.iterrows():\n",
    " if type(row[1]) == float:\n",
    "   cleanup.append(index)\n",
    "\n",
    "for ind in cleanup:\n",
    " data = data.drop(labels = ind, axis = 0)\n",
    "\n",
    "cleanup2 = []\n",
    "for index,row in data.iterrows():\n",
    " if type(row[1]) == float:\n",
    "   cleanup2.append(index)\n",
    "\n",
    "for ind in cleanup2:\n",
    " data = data.drop(labels = ind, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "SPECIAL_TOKENS_DICT = {\n",
    "    'pad_token': '<pad>',\n",
    "    'additional_special_tokens': ['<context>', '<character>'],\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, seq_length=64):\n",
    "\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "    character_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "    pad_tkn = tokenizer.pad_token_id\n",
    "    eos_tkn = tokenizer.eos_token_id\n",
    "\n",
    "    self.examples = []\n",
    "    for index, row in data.iterrows():\n",
    "      # Build the context and character segments:\n",
    "      context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length // 2 - 1)\n",
    "      character = [character_tkn] + tokenizer.encode(row[1], max_length=seq_length // 2 - 2) + [eos_tkn]\n",
    "\n",
    "      # Concatenate the two parts together:\n",
    "      tokens = context + character + [pad_tkn] * (seq_length - len(context) - len(character))\n",
    "\n",
    "      # Annotate each token with its corresponding segment:\n",
    "      segments = [context_tkn] * len(context) + [character_tkn] * (seq_length - len(context))\n",
    "\n",
    "      # Ignore the context, padding, and <character> tokens by setting their labels to -1\n",
    "      labels = [-100] * (len(context) + 1) + character[1:] + [-100] * (seq_length - len(context) - len(character))\n",
    "\n",
    "      # Add the preprocessed example to the dataset\n",
    "      self.examples.append((tokens, segments, labels))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return torch.tensor(self.examples[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "character_dataset = CharacterDataset(data, tokenizer)\n",
    "\n",
    "# Create data indices for training and validation splits:\n",
    "\n",
    "indices = list(range(len(character_dataset)))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "split = math.floor(0.1 * len(character_dataset))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Build the PyTorch data loaders:\n",
    "torch.manual_seed(0)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(character_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(character_dataset, batch_size=64, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cuda')):\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print('\\n--- Starting epoch #{} ---'.format(i))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # These 2 lists will keep track of the batch losses and batch sizes over one epoch:\n",
    "    losses = []\n",
    "    nums = []\n",
    "\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\n",
    "      # Move the batch to the training device:\n",
    "\n",
    "      inputs = xb.to(device)\n",
    "\n",
    "      # Call the model with the token ids, segment ids, and the ground truth (labels)\n",
    "      outputs = model(inputs[:, 0, :], token_type_ids=inputs[:, 1, :], labels=inputs[:, 2, :])\n",
    "      # print(outputs)\n",
    "      # Add the loss and batch size to the list:\n",
    "      loss = outputs[0]\n",
    "      losses.append(loss.item())\n",
    "      nums.append(len(xb))\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      model.zero_grad()\n",
    "\n",
    "    # Compute the average cost over one epoch:\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "    # Now do the same thing for validation:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      losses = []\n",
    "      nums = []\n",
    "\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\n",
    "        # inputs = xb.to(device)\n",
    "        outputs = model(inputs[:, 0, :], token_type_ids=inputs[:, 1, :], labels=inputs[:, 2, :])\n",
    "        losses.append(outputs[0].item())\n",
    "        nums.append(len(xb))\n",
    "\n",
    "\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting epoch #0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 27/27 [00:04<00:00,  5.98it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #0 finished --- Training cost: 1.431231556772904 / Validation cost: 0.8193696141242981\n",
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 27/27 [00:04<00:00,  6.03it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 1.025867523594669 / Validation cost: 0.44665107131004333\n",
      "\n",
      "--- Starting epoch #2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 27/27 [00:04<00:00,  6.47it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00, 20.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #2 finished --- Training cost: 0.7234174454184007 / Validation cost: 0.3510701656341553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the GPU:\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune GPT2 for two epochs:\n",
    "optimizer = AdamW(model.parameters())\n",
    "optimizer.step()\n",
    "fit(model, optimizer, train_loader, val_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "  \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "      Args:\n",
    "          logits: logits distribution shape (batch size x vocabulary size)\n",
    "          top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "          top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "              Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "      From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "  \"\"\"\n",
    "  top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "  if top_k > 0:\n",
    "    # Remove all tokens with a probability less than the last token of the top-k\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "  if top_p > 0.0:\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Shift the indices to the right to keep also the first token above the threshold\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    # scatter sorted tensors to original indexing\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    logits[indices_to_remove] = filter_value\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From HuggingFace, adapted to work with the context/character separation:\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0,\n",
    "                    repetition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "  context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "  context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "  generated = context\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for _ in trange(length):\n",
    "\n",
    "      inputs = {'input_ids': generated}\n",
    "      if segments_tokens != None:\n",
    "        inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples,\n",
    "                                                                                                          1)\n",
    "\n",
    "      outputs = model(\n",
    "        **inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "      next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "      # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "      for i in range(num_samples):\n",
    "        for _ in set(generated[i].tolist()):\n",
    "          next_token_logits[i, _] /= repetition_penalty\n",
    "\n",
    "      filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "      if temperature == 0:  # greedy sampling:\n",
    "        next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "      else:\n",
    "        next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "      generated = torch.cat((generated, next_token), dim=1)\n",
    "  return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"IceKingModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"IceKingModel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'character_tkn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35580\\611235062.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcontext_tkn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msegments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcharacter_tkn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0msegments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcontext_tkn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'character_tkn' is not defined"
     ]
    }
   ],
   "source": [
    "context = \"Who are you?\"\n",
    "\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "character_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\n",
    "\n",
    "segments = [character_tkn] * 64\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n",
    "\n",
    "input_ids += [character_tkn]\n",
    "\n",
    "# Move the model back to the CPU for inference:\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "# Generate 20 samples of max length 20\n",
    "generated = sample_sequence(\n",
    "  model,\n",
    "  length=20,\n",
    "  context=input_ids,\n",
    "  segments_tokens=segments,\n",
    "  num_samples=20,\n",
    "  temperature=1,\n",
    "  top_k=0,\n",
    "  top_p=0\n",
    ")\n",
    "\n",
    "for g in generated:\n",
    "  character = tokenizer.decode(g.squeeze().tolist())\n",
    "  character = character.split('<|endoftext|>')[0]\n",
    "  character = character.split('<context>')[1]\n",
    "  character = character.split('<character>')\n",
    "  print(character[0], character[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ced6e71201467cb53d560a5df0d8d1c74964a9d8a3ffef2f186fea0a0895ee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
